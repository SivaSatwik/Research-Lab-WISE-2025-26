{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Preprocessing\n",
    "\n",
    "**Project:** Trust, Ethical Concerns, and Usage Patterns of ChatGPT  \n",
    "**Team:** Team 5  \n",
    "**Date:** November 2025\n",
    "\n",
    "This notebook loads the raw dataset, performs cleaning, creates composite scores, and saves cleaned data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Set style for plots\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"✅ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_excel('../data/raw.xlsx')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Rows: {len(df):,}\")\n",
    "print(f\"Columns: {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display column names\n",
    "print(\"All columns:\")\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f\"{i:3d}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"Duplicate rows: {duplicates}\")\n",
    "\n",
    "if duplicates > 0:\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"✅ Removed {duplicates} duplicates\")\n",
    "    print(f\"New shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing data\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing': missing,\n",
    "    'Percent': missing_pct\n",
    "}).sort_values('Percent', ascending=False)\n",
    "\n",
    "print(\"Top 20 columns with missing data:\")\n",
    "print(missing_df.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Composite Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Perceived Capabilities (Q22a-j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define capability items\n",
    "capability_cols = [f'Q22{x}' for x in ['a','b','c','d','e','f','g','h','i','j']]\n",
    "existing_caps = [col for col in capability_cols if col in df.columns]\n",
    "\n",
    "print(f\"Capability columns found: {len(existing_caps)}/10\")\n",
    "print(existing_caps)\n",
    "\n",
    "# Create composite score\n",
    "if existing_caps:\n",
    "    df['capabilities_score'] = df[existing_caps].mean(axis=1)\n",
    "    \n",
    "    print(f\"\\n✅ Capabilities score created\")\n",
    "    print(f\"Mean: {df['capabilities_score'].mean():.2f}\")\n",
    "    print(f\"SD: {df['capabilities_score'].std():.2f}\")\n",
    "    print(f\"Range: [{df['capabilities_score'].min():.2f}, {df['capabilities_score'].max():.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(df['capabilities_score'].dropna(), bins=30, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Capabilities Score (1-5)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Perceived Capabilities Score')\n",
    "plt.axvline(df['capabilities_score'].mean(), color='red', linestyle='--', label='Mean')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Ethical Concerns (Q28a-i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ethics items\n",
    "ethics_cols = [f'Q28{x}' for x in ['a','b','c','d','e','f','g','h','i']]\n",
    "existing_ethics = [col for col in ethics_cols if col in df.columns]\n",
    "\n",
    "print(f\"Ethics columns found: {len(existing_ethics)}/9\")\n",
    "print(existing_ethics)\n",
    "\n",
    "# Create composite score\n",
    "if existing_ethics:\n",
    "    df['ethics_concerns_score'] = df[existing_ethics].mean(axis=1)\n",
    "    \n",
    "    print(f\"\\n✅ Ethical concerns score created\")\n",
    "    print(f\"Mean: {df['ethics_concerns_score'].mean():.2f}\")\n",
    "    print(f\"SD: {df['ethics_concerns_score'].std():.2f}\")\n",
    "    print(f\"Range: [{df['ethics_concerns_score'].min():.2f}, {df['ethics_concerns_score'].max():.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(df['ethics_concerns_score'].dropna(), bins=30, edgecolor='black', alpha=0.7, color='orange')\n",
    "plt.xlabel('Ethical Concerns Score (1-5)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Ethical Concerns Score')\n",
    "plt.axvline(df['ethics_concerns_score'].mean(), color='red', linestyle='--', label='Mean')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Attitudes (Q21a-d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define attitude items\n",
    "attitude_cols = [f'Q21{x}' for x in ['a','b','c','d']]\n",
    "existing_attitudes = [col for col in attitude_cols if col in df.columns]\n",
    "\n",
    "print(f\"Attitude columns found: {len(existing_attitudes)}/4\")\n",
    "print(existing_attitudes)\n",
    "\n",
    "# Create composite score\n",
    "if existing_attitudes:\n",
    "    df['attitudes_score'] = df[existing_attitudes].mean(axis=1)\n",
    "    \n",
    "    print(f\"\\n✅ Attitudes score created\")\n",
    "    print(f\"Mean: {df['attitudes_score'].mean():.2f}\")\n",
    "    print(f\"SD: {df['attitudes_score'].std():.2f}\")\n",
    "    print(f\"Range: [{df['attitudes_score'].min():.2f}, {df['attitudes_score'].max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Usage Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map usage frequency\n",
    "if 'Q8' in df.columns:\n",
    "    usage_map = {1.0: 'Daily', 2.0: 'Weekly', 3.0: 'Monthly'}\n",
    "    df['usage_label'] = df['Q8'].map(usage_map)\n",
    "    \n",
    "    print(\"Usage Frequency Distribution:\")\n",
    "    print(df['usage_label'].value_counts())\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    df['usage_label'].value_counts().plot(kind='bar', color=['green', 'blue', 'orange'])\n",
    "    plt.xlabel('Usage Frequency')\n",
    "    plt.ylabel('Number of Students')\n",
    "    plt.title('ChatGPT Usage Frequency Distribution')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Verify Key Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check key variables\n",
    "key_vars = {\n",
    "    'Trust (Q15)': 'Q15',\n",
    "    'Recommendation (Q16)': 'Q16',\n",
    "    'Usage (Q8)': 'Q8',\n",
    "    'Capabilities': 'capabilities_score',\n",
    "    'Ethics Concerns': 'ethics_concerns_score',\n",
    "    'Attitudes': 'attitudes_score'\n",
    "}\n",
    "\n",
    "print(\"Key Variables Summary:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for name, var in key_vars.items():\n",
    "    if var in df.columns:\n",
    "        missing = df[var].isna().sum()\n",
    "        missing_pct = (missing / len(df)) * 100\n",
    "        print(f\"{name:20s} - Missing: {missing:5d} ({missing_pct:5.1f}%)\")\n",
    "    else:\n",
    "        print(f\"{name:20s} - ❌ NOT FOUND\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Reliability Analysis (Cronbach's Alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import pingouin as pg\n",
    "    \n",
    "    print(\"Cronbach's Alpha (Reliability):\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Capabilities\n",
    "    if existing_caps:\n",
    "        alpha_cap = pg.cronbach_alpha(data=df[existing_caps].dropna())\n",
    "        print(f\"Capabilities Scale: α = {alpha_cap[0]:.3f} {'✅' if alpha_cap[0] > 0.70 else '⚠️'}\")\n",
    "    \n",
    "    # Ethics\n",
    "    if existing_ethics:\n",
    "        alpha_eth = pg.cronbach_alpha(data=df[existing_ethics].dropna())\n",
    "        print(f\"Ethics Scale:       α = {alpha_eth[0]:.3f} {'✅' if alpha_eth[0] > 0.70 else '⚠️'}\")\n",
    "    \n",
    "    # Attitudes\n",
    "    if existing_attitudes:\n",
    "        alpha_att = pg.cronbach_alpha(data=df[existing_attitudes].dropna())\n",
    "        print(f\"Attitudes Scale:    α = {alpha_att[0]:.3f} {'✅' if alpha_att[0] > 0.70 else '⚠️'}\")\n",
    "    \n",
    "    print(\"\\nNote: α > 0.70 indicates acceptable reliability\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"⚠️ pingouin not installed. Run: pip install pingouin\")\n",
    "    print(\"   Skipping reliability analysis for now.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned dataset\n",
    "output_path = '../data/cleaned/cleaned_data.csv'\n",
    "\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"✅ Cleaned data saved to: {output_path}\")\n",
    "print(f\"   Shape: {df.shape}\")\n",
    "print(f\"   Size: {Path(output_path).stat().st_size / 1024 / 1024:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"DATA CLEANING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\"\"\n",
    "Final Dataset:\n",
    "- Rows: {len(df):,}\n",
    "- Columns: {len(df.columns)}\n",
    "\n",
    "Composite Scores Created:\n",
    "- Capabilities (Q22a-j): {'✅' if 'capabilities_score' in df.columns else '❌'}\n",
    "- Ethical Concerns (Q28a-i): {'✅' if 'ethics_concerns_score' in df.columns else '❌'}\n",
    "- Attitudes (Q21a-d): {'✅' if 'attitudes_score' in df.columns else '❌'}\n",
    "\n",
    "Key Variables:\n",
    "- Trust (Q15): {'✅' if 'Q15' in df.columns else '❌'}\n",
    "- Usage (Q8): {'✅' if 'Q8' in df.columns else '❌'}\n",
    "- Recommendation (Q16): {'✅' if 'Q16' in df.columns else '❌'}\n",
    "\n",
    "✅ Data cleaning complete!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. Run **02_descriptive_statistics.ipynb** for summary stats\n",
    "2. Run **03_correlation_analysis.ipynb** for RQ3\n",
    "3. Run **04_regression_analysis.ipynb** for RQ1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
